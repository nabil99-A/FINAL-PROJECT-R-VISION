{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c35077-2466-412e-b059-40cb9b0e6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Body detection: model is trained as me, myself character so u can get along with the body files,capturing,training procedures... in vision99 code .  \n",
    "#Lip detection: \n",
    "#   character base videos and aligns link https://spandh.dcs.shef.ac.uk//gridcorpus/  u can choose and download data from this website\n",
    "#   download modelweight-checkpoint file of lip detection and get them as :\n",
    "\n",
    "output = 'checkpoints.zip'\n",
    "gdown.extractall('checkpoints.zip', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d80b3a7-c232-49fb-9345-e9b3ff6d461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vision-loadmodelweight.py\n"
     ]
    }
   ],
   "source": [
    "#save as py to import from it in streamlit code\n",
    "%%writefile vision-loadmodelweight.py\n",
    "\n",
    "import os \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "\n",
    "def load_model() -> Sequential: \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "    model.add(Conv3D(256, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "    model.add(Conv3D(75, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Dense(41, kernel_initializer='he_normal', activation='softmax'))\n",
    "\n",
    "    model.load_weights(os.path.join('models','checkpoint'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b59ccb5-e5ec-44bc-8c32-de78a178dbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vision-loadfunctions.py\n"
     ]
    }
   ],
   "source": [
    "#save as py to import from it in streamlit code\n",
    "%%writefile vision-loadfunctions.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "def load_video(path:str) -> List[float]: \n",
    "    #print(path)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))): \n",
    "        ret, frame = cap.read()\n",
    "        frame = tf.image.rgb_to_grayscale(frame)\n",
    "        frames.append(frame[190:236,80:220,:])\n",
    "    cap.release()\n",
    "    \n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std\n",
    "    \n",
    "def load_alignments(path:str) -> List[str]: \n",
    "    #print(path)\n",
    "    with open(path, 'r') as f: \n",
    "        lines = f.readlines() \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil': \n",
    "            tokens = [*tokens,' ',line[2]]\n",
    "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]\n",
    "\n",
    "def load_data(path: str): \n",
    "    path = bytes.decode(path.numpy())\n",
    "    #file_name = path.split('/')[-1].split('.')[0]\n",
    "    # File name splitting for windows\n",
    "    file_name = path.split('\\\\')[-1].split('.')[0]\n",
    "    video_path = os.path.join('data','s1',f'{file_name}.mpg')\n",
    "    alignment_path = os.path.join('data','alignments','s1',f'{file_name}.align')\n",
    "    frames = load_video(video_path) \n",
    "    alignments = load_alignments(alignment_path)\n",
    "    \n",
    "    return frames, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c1a418-bf27-4b75-888e-6e90e5839f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vision9c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vision9c.py\n",
    "import subprocess\n",
    "from subprocess import run\n",
    "import time\n",
    "from typing import List\n",
    "import gdown\n",
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os \n",
    "import imageio \n",
    "from vision-loadfunctions import load_data, num_to_char\n",
    "from vision-loadmodelweight import load_model\n",
    "\n",
    "# Initialize MediaPipe holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def convert_video_to_compatible_format(input_video_path):\n",
    "    output_video_path = os.path.splitext(input_video_path)[0] + '_compatible.mp4'\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_video_path,\n",
    "        '-c:v', 'libx264',\n",
    "        '-preset', 'fast',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        output_video_path,\n",
    "        '-y'\n",
    "    ]\n",
    "    try:\n",
    "        run(command, check=True)\n",
    "        return output_video_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        st.error(f\"FFmpeg error: {e}\")\n",
    "        return None\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "def draw_styled_landmarks(image, results):    \n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )     \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )     \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "model = tf.keras.models.load_model('body.h5')\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['neutral', 'shifting', 'pausing'])\n",
    "\n",
    "# Process video and apply action recognition\n",
    "def process_video(video_file):\n",
    "    # Convert the uploaded video to a compatible format\n",
    "    converted_video_path = convert_video_to_compatible_format(video_file.name)\n",
    "    if not converted_video_path:\n",
    "        st.error(\"Video conversion failed. Please check the video format.\")\n",
    "        st.stop()\n",
    "\n",
    "    cap = cv2.VideoCapture(converted_video_path)\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    predictions = []\n",
    "    threshold = 0.5\n",
    "    colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "    processed_frames_with_labels = []\n",
    "\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]  # Keep last 30 frames\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                predictions.append(np.argmax(res))\n",
    "                \n",
    "                # Visualization logic\n",
    "                if np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                    if res[np.argmax(res)] > threshold: \n",
    "                        if len(sentence) > 0: \n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Viz probabilities and add action labels to the frame\n",
    "                image_with_labels = prob_viz(res, actions, image, colors)\n",
    "                processed_frames_with_labels.append(image_with_labels)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    # Check if the list is not empty before accessing\n",
    "    if processed_frames_with_labels:\n",
    "        height, width, layers = processed_frames_with_labels[0].shape\n",
    "        size = (width, height)\n",
    "        \n",
    "        # Define the codec and create VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        processed_video_path = 'processed_video_with_actions.mp4'\n",
    "        out = cv2.VideoWriter(processed_video_path, fourcc, 15, size)\n",
    "        \n",
    "        for frame in processed_frames_with_labels:\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "        \n",
    "        # Convert the processed video to a compatible format using ffmpeg\n",
    "        final_video_path = 'final_video_with_actions.mp4'\n",
    "        run(['ffmpeg', '-i', processed_video_path, '-vcodec', 'libx264', final_video_path, '-y'])\n",
    "        \n",
    "        # Remove the temporary processed video file\n",
    "        os.remove(processed_video_path)\n",
    "        \n",
    "        return final_video_path\n",
    "    else:\n",
    "        st.error(\"No frames were processed. Please check the video file and try again.\")\n",
    "        return None\n",
    "#############################################################################################################################################        \n",
    "st.set_page_config(layout='wide')\n",
    "\n",
    "with st.sidebar: \n",
    "    \n",
    "    st.title('ROBOTIC-VISION')\n",
    "    st.info('This application is originally for developing smart analyses systems.')\n",
    "\n",
    "# Initialize session states for each tab if not already present\n",
    "if 'tab1_state' not in st.session_state:\n",
    "    st.session_state['tab1_state'] = None\n",
    "\n",
    "if 'tab2_state' not in st.session_state:\n",
    "    st.session_state['tab2_state'] = None\n",
    "\n",
    "# Define the sidebar options\n",
    "sidebar_options = [\"Lip detection\", \"Body detection\"]\n",
    "selected_option = st.sidebar.selectbox(\"Choose an option:\", sidebar_options)\n",
    "\n",
    "# Define a sidebar button for capture\n",
    "if st.sidebar.button(\"Live Body Detection\"):\n",
    "    st.session_state['Live Body Detection'] = True\n",
    "else:\n",
    "    st.session_state['Live Body Detection'] = False\n",
    "\n",
    "if selected_option == \"Lip detection\":\n",
    "    st.session_state['tab1_state'] = \"active\"\n",
    "    st.session_state['tab2_state'] = None\n",
    "    st.title('Lip Reading and Analysis')  \n",
    "    options = os.listdir(os.path.join('data', 's1'))\n",
    "    selected_video = st.selectbox('Choose video for lip reading', options, key='video_selection_tab1')\n",
    "    if options:\n",
    "        st.info('The video below displays the converted video in mp4 format for lip reading')\n",
    "        file_path = os.path.join('data', 's1', selected_video)\n",
    "        converted_video_path = convert_video_to_compatible_format(file_path)\n",
    "        if converted_video_path:\n",
    "            video_file = open(converted_video_path, 'rb')\n",
    "            video_bytes = video_file.read()\n",
    "            st.video(video_bytes)\n",
    "            video_file.close()  # Close the file after reading\n",
    "\n",
    "        st.info('This is all the machine learning model sees when making a prediction for lip reading')\n",
    "        video, annotations = load_data(tf.convert_to_tensor(file_path))\n",
    "        \n",
    "        st.image('animation.gif', width=400) \n",
    "        st.info('This is the output of the machine learning model as tokens for lip reading')\n",
    "        model = load_model()\n",
    "        yhat = model.predict(tf.expand_dims(video, axis=0))\n",
    "        decoder = tf.keras.backend.ctc_decode(yhat, [75], greedy=True)[0][0].numpy()\n",
    "        st.text(decoder)\n",
    "        # Convert prediction to text\n",
    "        st.info('Decode the raw tokens into words for lip reading')\n",
    "        converted_prediction = tf.strings.reduce_join(num_to_char(decoder)).numpy().decode('utf-8')\n",
    "        st.text(converted_prediction)\n",
    "\n",
    "elif selected_option == \"Body detection\":\n",
    "    st.session_state['tab1_state'] = None\n",
    "    st.session_state['tab2_state'] = \"active\"\n",
    "    st.info('The video below displays the processed video with landmarks and actions')\n",
    "    uploaded_video = st.file_uploader(\"Upload a video\", type=[\"mp4\", \"mpg\", \"mpeg...\"])\n",
    "    if uploaded_video is not None:\n",
    "        # Save the uploaded video to a temporary file\n",
    "        with open(uploaded_video.name, \"wb\") as f:\n",
    "            f.write(uploaded_video.getbuffer())\n",
    "        \n",
    "        # Process video and display processed video with landmarks and actions\n",
    "        processed_video_path = process_video(uploaded_video)\n",
    "        if processed_video_path:\n",
    "            st.video(processed_video_path)\n",
    "\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(uploaded_video.name):\n",
    "            os.remove(uploaded_video.name)\n",
    "        if processed_video_path and os.path.exists(processed_video_path):\n",
    "            os.remove(processed_video_path)\n",
    "            \n",
    "if st.session_state['Live Body Detection']:\n",
    "    def ld_model(model_path):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        return model\n",
    "    \n",
    "    # Later, when you want to use the model\n",
    "    model_path = 'body.h5'  # Specify the correct path to your model weights\n",
    "    loaded_model = ld_model(model_path)\n",
    "    # Actions that we try to detect\n",
    "    actions = np.array(['neutral', 'shifting', 'pausing'])\n",
    "    colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    predictions = []\n",
    "    threshold = 0.5  \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "    \n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "    \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "    \n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                res = loaded_model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                predictions.append(np.argmax(res))\n",
    "    \n",
    "                # 3. Viz logic\n",
    "                if len(predictions) >= 10 and np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                    if res[np.argmax(res)] > threshold: \n",
    "                        \n",
    "                        action = actions[np.argmax(res)]\n",
    "                        if action == 'pausing':\n",
    "                            display_action = 'off'\n",
    "                        elif action == 'shifting':\n",
    "                            display_action = 'on'\n",
    "                        else:\n",
    "                            display_action = action\n",
    "    \n",
    "                        if len(sentence) > 0: \n",
    "                            if display_action != sentence[-1]:\n",
    "                                sentence.append(display_action)\n",
    "                        else:\n",
    "                            sentence.append(display_action)\n",
    "        \n",
    "                    if len(sentence) > 5: \n",
    "                        sentence = sentence[-5:]\n",
    "        \n",
    "                    # Viz probabilities\n",
    "                    image = prob_viz(res, actions, image, colors)\n",
    "                    \n",
    "                cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "                cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1fd24c-468b-4856-8a3e-0241228c26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! streamlit run vision9c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94955d4b-4b76-4684-ab84-d4da4e276893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
